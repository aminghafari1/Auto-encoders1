{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## building an auto-encoder class\n",
    "## builiding the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m     autoencoder\u001b[39m=\u001b[39m Autoencoder(input_shape\u001b[39m=\u001b[39;49m[\u001b[39m28\u001b[39;49m,\u001b[39m28\u001b[39;49m,\u001b[39m1\u001b[39;49m], conv_filters\u001b[39m=\u001b[39;49m[\u001b[39m32\u001b[39;49m,\u001b[39m64\u001b[39;49m,\u001b[39m64\u001b[39;49m,\u001b[39m64\u001b[39;49m], conv_kernels\u001b[39m=\u001b[39;49m[\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m], conv_strides\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m1\u001b[39;49m], latent_space_dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m    128\u001b[0m     autoencoder\u001b[39m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn [5], line 23\u001b[0m, in \u001b[0;36mAutoencoder.__init__\u001b[1;34m(self, input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_conv_layers\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(conv_filters)\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape_before_bottleneck\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build()\n",
      "Cell \u001b[1;32mIn [5], line 31\u001b[0m, in \u001b[0;36mAutoencoder._build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_encoder()\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_decoder()\n",
      "Cell \u001b[1;32mIn [5], line 40\u001b[0m, in \u001b[0;36mAutoencoder._build_decoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m conv_transpose_layers\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_conv_transpose_layers(reshape_layer)\n\u001b[0;32m     39\u001b[0m decoder_output\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_decoder_output(conv_transpose_layers)\n\u001b[1;32m---> 40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m=\u001b[39mModel(decoder_input, decoder_output)\n",
      "File \u001b[1;32mc:\\Users\\aghaffari\\Desktop\\Python\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aghaffari\\Desktop\\Python\\lib\\site-packages\\keras\\engine\\functional.py:166\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    158\u001b[0m         [\n\u001b[0;32m    159\u001b[0m             functional_utils\u001b[39m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    160\u001b[0m             \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[0;32m    161\u001b[0m         ]\n\u001b[0;32m    162\u001b[0m     ):\n\u001b[0;32m    163\u001b[0m         inputs, outputs \u001b[39m=\u001b[39m functional_utils\u001b[39m.\u001b[39mclone_graph_nodes(\n\u001b[0;32m    164\u001b[0m             inputs, outputs\n\u001b[0;32m    165\u001b[0m         )\n\u001b[1;32m--> 166\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_graph_network(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\aghaffari\\Desktop\\Python\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aghaffari\\Desktop\\Python\\lib\\site-packages\\keras\\engine\\functional.py:208\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m    204\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39m_keras_history\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\n\u001b[0;32m    205\u001b[0m     ):\n\u001b[0;32m    206\u001b[0m         base_layer_utils\u001b[39m.\u001b[39mcreate_keras_history(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_outputs)\n\u001b[1;32m--> 208\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_graph_inputs_and_outputs()\n\u001b[0;32m    210\u001b[0m \u001b[39m# A Network does not create weights of its own, thus it is already\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m# built.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aghaffari\\Desktop\\Python\\lib\\site-packages\\keras\\engine\\functional.py:859\u001b[0m, in \u001b[0;36mFunctional._validate_graph_inputs_and_outputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m_keras_history\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    858\u001b[0m     cls_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m--> 859\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    860\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOutput tensors of a \u001b[39m\u001b[39m{\u001b[39;00mcls_name\u001b[39m}\u001b[39;00m\u001b[39m model must be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    861\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe output of a TensorFlow `Layer` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    862\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(thus holding past layer metadata). Found: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    863\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: None"
     ]
    }
   ],
   "source": [
    "from keras import Model \n",
    "from keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "class Autoencoder:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "        self.input_shape = input_shape # [28,28,1] this means the input is a 28x28 image with 1 channel (grayscale) like mnist\n",
    "        self.conv_filters = conv_filters #[2,4,8] this means the first layer has two filters, the second 4, and the last 8.\n",
    "        self.conv_kernels = conv_kernels #[3,5,3] this means the first layer has a 3x3 kernel, the second 5x5, and the last 3x3.\n",
    "        self.conv_strides= conv_strides  #[1,2,2] this means the first layer has a stride of 1, the second 2, and the last 2.\n",
    "        self.latent_space_dim = latent_space_dim # 2 this means the bottleneck will only have 2 dimensions.\n",
    "        self.encoder= None\n",
    "        self.decoder = None\n",
    "        self.model= None\n",
    "        self._num_conv_layers= len(conv_filters)\n",
    "        self._shape_before_bottleneck=None\n",
    "        self._build()\n",
    "        \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "        \n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        #self._build_autoencoder()\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        decoder_input= self._add_decoder_input()\n",
    "        dense_layer= self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "        conv_transpose_layers= self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output= self._add_decoder_output(conv_transpose_layers)\n",
    "        self.decoder=Model(decoder_input, decoder_output, name='decoder')\n",
    "        \n",
    "    def _add_decoder_input(self):\n",
    "        return Input(shape=(self.latent_space_dim,), name='decoder_input')\n",
    "    \n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(self._shape_before_bottleneck)\n",
    "        ## This will multuply all the elements in the list, because the shape before bottleneck has like 3 elements\n",
    "        dense_layer = Dense(num_neurons, name='decoder_dense')(decoder_input)\n",
    "        return dense_layer\n",
    "    \n",
    "    def _add_reshape_layer(self, dense_layer):\n",
    "        ## we want to go back to the three d layer, we want to go the the encoder shape before flattening\n",
    "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
    "    \n",
    "    def _add_conv_transpose_layers(self, x):\n",
    "        ## Add convolutional transpose blocks\n",
    "        ## Loop through all the conv layers in reverse order and stop at the first layer\n",
    "        ## we need to ignore the first convolutional layer, and we want to do it in reverse order\n",
    "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
    "            #[0,1,2] ->[2,1]\n",
    "            x= self._add_conv_transpose_layer(layer_index, x)\n",
    "        return x\n",
    "    \n",
    "    def _add_conv_transpose_layer(self, layer_index, x):\n",
    "        layer_num = self._num_conv_layers - layer_index\n",
    "        conv_transpose_layer= Conv2DTranspose(filters=self.conv_filters[layer_index],\n",
    "                                              kernel_size=self.conv_kernels[layer_index],\n",
    "                                                strides=self.conv_strides[layer_index],\n",
    "                                                padding='same',\n",
    "                                                name=f'decoder_conv_transpose_{layer_num}')\n",
    "        x= conv_transpose_layer(x)\n",
    "        x=ReLU(name=f'decoder_relu_{layer_num}')(x)\n",
    "        x= BatchNormalization(name=f'decoder_bn_{layer_num}')(x)\n",
    "        return x\n",
    "         \n",
    "    def _add_decoder_output(self, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(filters=1, ## [24 24 1] height and width and num of channels we set to one because of number of channels in mnist\n",
    "                                              kernel_size=self.conv_kernels[0], ## getting the first kernel size\n",
    "                                                strides=self.conv_strides[0],\n",
    "                                                padding='same',\n",
    "                                                name=f'decoder_conv_transpose_layer_{self._num_conv_layers}')\n",
    "        x= conv_transpose_layer(x)\n",
    "        output_layer = Activation('sigmoid', name='sigmoid_layer')(x)\n",
    "         \n",
    "         \n",
    "    \n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input()\n",
    "        conv_layers= self._add_conv_layers(encoder_input)\n",
    "        bottleneck = self._add_bottleneck(conv_layers)\n",
    "        self.encoder = Model(encoder_input, bottleneck, name='encoder')\n",
    "    \n",
    "    def _add_encoder_input(self):\n",
    "        return Input(shape=self.input_shape, name='encoder_input')\n",
    "        \n",
    "    ## This will create all convolutional blocks in encoder    \n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        x= encoder_input\n",
    "        for layer_index in range(self._num_conv_layers):\n",
    "            x = self._add_conv_layer(layer_index, x)\n",
    "        return x   \n",
    "    \n",
    "    def _add_conv_layer(self, layer_index, x):\n",
    "        layer_number= layer_index+1\n",
    "        ## Adds a convolutional block to a graph of layers, consisting of a conv 2d+ batch norm + relu \n",
    "        conv_layer = Conv2D(filters=self.conv_filters[layer_index],kernel_size=self.conv_kernels[layer_index],\n",
    "                            strides=self.conv_strides[layer_index], \n",
    "                            padding='same', name=f'encoder_conv_layer_{layer_number}')\n",
    "        x= conv_layer(x)\n",
    "        x= ReLU(name=f'encoder_relu_{layer_number}')(x)\n",
    "        x= BatchNormalization(name=f'encoder_bn_{layer_number}')(x)\n",
    "        return x\n",
    "    \n",
    " \n",
    "    \n",
    "    def _add_bottleneck(self, x):\n",
    "        ## Flatten the data and add bottleneck (a dense layer)\n",
    "        ## we want to store the shaoe of the data before flattening it, so we can use it in the decoder\n",
    "        self._shape_before_bottleneck= K.int_shape(x)[1:] #in our case [2 7 7 32] first -> batch size, second and third -> spatial dimensions, fourth -> number of channels\n",
    "        x= Flatten()(x)\n",
    "        ## Here it is like we instantiate a dense layer (the first paranthesis) and then we call it on x (with the second paranthesis)\n",
    "        x= Dense(self.latent_space_dim, name='encoder_output')(x)\n",
    "        return x\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    autoencoder= Autoencoder(input_shape=[28,28,1], conv_filters=[32,64,64,64], conv_kernels=[3,3,3,3], conv_strides=[1,2,2,1], latent_space_dim=2)\n",
    "    autoencoder.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## moving to the decoder\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfc00275c037e0322d65458631b9d66cd4b1daeaa2d39a92fa6d181b0d340908"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
