{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## building an auto-encoder class\n",
    "## builiding the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder_conv_layer_1 (Conv2  (None, 28, 28, 32)       320       \n",
      " D)                                                              \n",
      "                                                                 \n",
      " encoder_relu_1 (ReLU)       (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " encoder_bn_1 (BatchNormaliz  (None, 28, 28, 32)       128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " encoder_conv_layer_2 (Conv2  (None, 14, 14, 64)       18496     \n",
      " D)                                                              \n",
      "                                                                 \n",
      " encoder_relu_2 (ReLU)       (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " encoder_bn_2 (BatchNormaliz  (None, 14, 14, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " encoder_conv_layer_3 (Conv2  (None, 7, 7, 64)         36928     \n",
      " D)                                                              \n",
      "                                                                 \n",
      " encoder_relu_3 (ReLU)       (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " encoder_bn_3 (BatchNormaliz  (None, 7, 7, 64)         256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " encoder_conv_layer_4 (Conv2  (None, 7, 7, 64)         36928     \n",
      " D)                                                              \n",
      "                                                                 \n",
      " encoder_relu_4 (ReLU)       (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " encoder_bn_4 (BatchNormaliz  (None, 7, 7, 64)         256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " encoder_output (Dense)      (None, 2)                 6274      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 99,842\n",
      "Trainable params: 99,394\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " decoder_dense (Dense)       (None, 3136)              9408      \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " decoder_conv_transpose_1 (C  (None, 7, 7, 64)         36928     \n",
      " onv2DTranspose)                                                 \n",
      "                                                                 \n",
      " decoder_relu_1 (ReLU)       (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " decoder_bn_1 (BatchNormaliz  (None, 7, 7, 64)         256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_2 (C  (None, 14, 14, 64)       36928     \n",
      " onv2DTranspose)                                                 \n",
      "                                                                 \n",
      " decoder_relu_2 (ReLU)       (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " decoder_bn_2 (BatchNormaliz  (None, 14, 14, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_3 (C  (None, 28, 28, 64)       36928     \n",
      " onv2DTranspose)                                                 \n",
      "                                                                 \n",
      " decoder_relu_3 (ReLU)       (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " decoder_bn_3 (BatchNormaliz  (None, 28, 28, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 28, 28, 1)        577       \n",
      " r_4 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " sigmoid_layer (Activation)  (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,537\n",
      "Trainable params: 121,153\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 2)                 99842     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         121537    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221,379\n",
      "Trainable params: 220,547\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Model \n",
    "from keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "class Autoencoder:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "        self.input_shape = input_shape # [28,28,1] this means the input is a 28x28 image with 1 channel (grayscale) like mnist\n",
    "        self.conv_filters = conv_filters #[2,4,8] this means the first layer has two filters, the second 4, and the last 8.\n",
    "        self.conv_kernels = conv_kernels #[3,5,3] this means the first layer has a 3x3 kernel, the second 5x5, and the last 3x3.\n",
    "        self.conv_strides= conv_strides  #[1,2,2] this means the first layer has a stride of 1, the second 2, and the last 2.\n",
    "        self.latent_space_dim = latent_space_dim # 2 this means the bottleneck will only have 2 dimensions.\n",
    "        self.encoder= None\n",
    "        self.decoder = None\n",
    "        self.model= None\n",
    "        self._model_input = None\n",
    "        self._num_conv_layers= len(conv_filters)\n",
    "        self._shape_before_bottleneck=None\n",
    "        self._build()\n",
    "        \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "        self.model.summary()\n",
    "    \n",
    "    def compile(self, learning_rate=0.0001):\n",
    "        optimizer= Adam(learning_rate==learning_rate)\n",
    "        mse_loss = MeanSquaredError()\n",
    "        self.model.compile(optimizer=optimizer, loss=mse_loss)\n",
    "        \n",
    "    def train(self, x_train, batch_size, num_epochs):\n",
    "        self.model.fit(x_train, x_train, batch_size, epochs=num_epochs, shuffle=True)\n",
    "    \n",
    "        \n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        self._build_autoencoder() ##put together the encoder and the decoder\n",
    "        \n",
    "    def _build_autoencoder(self):\n",
    "        model_input= self._model_input\n",
    "        model_output = self.decoder(self.encoder(model_input))\n",
    "        self.model= Model(model_input, model_output, name='autoencoder')\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        decoder_input= self._add_decoder_input()\n",
    "        dense_layer= self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "        conv_transpose_layers= self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output= self._add_decoder_output(conv_transpose_layers)\n",
    "        self.decoder=Model(decoder_input, decoder_output, name='decoder')\n",
    "        \n",
    "    def _add_decoder_input(self):\n",
    "        return Input(shape=(self.latent_space_dim,), name='decoder_input')\n",
    "    \n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(self._shape_before_bottleneck)\n",
    "        ## This will multuply all the elements in the list, because the shape before bottleneck has like 3 elements\n",
    "        dense_layer = Dense(num_neurons, name='decoder_dense')(decoder_input)\n",
    "        return dense_layer\n",
    "    \n",
    "    def _add_reshape_layer(self, dense_layer):\n",
    "        ## we want to go back to the three d layer, we want to go the the encoder shape before flattening\n",
    "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
    "    \n",
    "    def _add_conv_transpose_layers(self, x):\n",
    "        ## Add convolutional transpose blocks\n",
    "        ## Loop through all the conv layers in reverse order and stop at the first layer\n",
    "        ## we need to ignore the first convolutional layer, and we want to do it in reverse order\n",
    "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
    "            #[0,1,2] ->[2,1]\n",
    "            x= self._add_conv_transpose_layer(layer_index, x)\n",
    "        return x\n",
    "    \n",
    "    def _add_conv_transpose_layer(self, layer_index, x):\n",
    "        layer_num = self._num_conv_layers - layer_index\n",
    "        conv_transpose_layer= Conv2DTranspose(filters=self.conv_filters[layer_index],\n",
    "                                              kernel_size=self.conv_kernels[layer_index],\n",
    "                                                strides=self.conv_strides[layer_index],\n",
    "                                                padding='same',\n",
    "                                                name=f'decoder_conv_transpose_{layer_num}')\n",
    "        x= conv_transpose_layer(x)\n",
    "        x=ReLU(name=f'decoder_relu_{layer_num}')(x)\n",
    "        x= BatchNormalization(name=f'decoder_bn_{layer_num}')(x)\n",
    "        return x\n",
    "         \n",
    "    def _add_decoder_output(self, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(filters=1, ## [24 24 1] height and width and num of channels we set to one because of number of channels in mnist\n",
    "                                              kernel_size=self.conv_kernels[0], ## getting the first kernel size\n",
    "                                                strides=self.conv_strides[0],\n",
    "                                                padding='same',\n",
    "                                                name=f'decoder_conv_transpose_layer_{self._num_conv_layers}')\n",
    "        x= conv_transpose_layer(x)\n",
    "        output_layer = Activation('sigmoid', name='sigmoid_layer')(x)\n",
    "        return output_layer\n",
    "         \n",
    "         \n",
    "    \n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input()\n",
    "        conv_layers= self._add_conv_layers(encoder_input)\n",
    "        bottleneck = self._add_bottleneck(conv_layers)\n",
    "        self._model_input= encoder_input\n",
    "        self.encoder = Model(encoder_input, bottleneck, name='encoder')\n",
    "    \n",
    "    def _add_encoder_input(self):\n",
    "        return Input(shape=self.input_shape, name='encoder_input')\n",
    "        \n",
    "    ## This will create all convolutional blocks in encoder    \n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        x= encoder_input\n",
    "        for layer_index in range(self._num_conv_layers):\n",
    "            x = self._add_conv_layer(layer_index, x)\n",
    "        return x   \n",
    "    \n",
    "    def _add_conv_layer(self, layer_index, x):\n",
    "        layer_number= layer_index+1\n",
    "        ## Adds a convolutional block to a graph of layers, consisting of a conv 2d+ batch norm + relu \n",
    "        conv_layer = Conv2D(filters=self.conv_filters[layer_index],kernel_size=self.conv_kernels[layer_index],\n",
    "                            strides=self.conv_strides[layer_index], \n",
    "                            padding='same', name=f'encoder_conv_layer_{layer_number}')\n",
    "        x= conv_layer(x)\n",
    "        x= ReLU(name=f'encoder_relu_{layer_number}')(x)\n",
    "        x= BatchNormalization(name=f'encoder_bn_{layer_number}')(x)\n",
    "        return x\n",
    "    \n",
    " \n",
    "    \n",
    "    def _add_bottleneck(self, x):\n",
    "        ## Flatten the data and add bottleneck (a dense layer)\n",
    "        ## we want to store the shaoe of the data before flattening it, so we can use it in the decoder\n",
    "        self._shape_before_bottleneck= K.int_shape(x)[1:] #in our case [2 7 7 32] first -> batch size, second and third -> spatial dimensions, fourth -> number of channels\n",
    "        x= Flatten()(x)\n",
    "        ## Here it is like we instantiate a dense layer (the first paranthesis) and then we call it on x (with the second paranthesis)\n",
    "        x= Dense(self.latent_space_dim, name='encoder_output')(x)\n",
    "        return x\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    autoencoder= Autoencoder(input_shape=[28,28,1], conv_filters=[32,64,64,64], conv_kernels=[3,3,3,3], conv_strides=[1,2,2,1], latent_space_dim=2)\n",
    "    autoencoder.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## moving to the decoder\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfc00275c037e0322d65458631b9d66cd4b1daeaa2d39a92fa6d181b0d340908"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
