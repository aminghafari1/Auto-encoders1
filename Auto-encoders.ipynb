{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## building an auto-encoder class\n",
    "## builiding the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model \n",
    "from keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, activation\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "class Autoencoder:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "        self.input_shape = input_shape # [28,28,1] this means the input is a 28x28 image with 1 channel (grayscale) like mnist\n",
    "        self.conv_filters = conv_filters #[2,4,8] this means the first layer has two filters, the second 4, and the last 8.\n",
    "        self.conv_kernels = conv_kernels #[3,5,3] this means the first layer has a 3x3 kernel, the second 5x5, and the last 3x3.\n",
    "        self.conv_strides= conv_strides  #[1,2,2] this means the first layer has a stride of 1, the second 2, and the last 2.\n",
    "        self.latent_space_dim = latent_space_dim # 2 this means the bottleneck will only have 2 dimensions.\n",
    "        self.encoder= None\n",
    "        self.decoder = None\n",
    "        self.model= None\n",
    "        self._num_conv_layers= len(conv_filters)\n",
    "        self._shape_before_bottleneck=None\n",
    "        self._build()\n",
    "        \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        \n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        #self._build_autoencoder()\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        decoder_input= self._add_decoder_input()\n",
    "        dense_layer= self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "        conv_transpose_layers= self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output= self._add_decoder_output(conv_transpose_layers)\n",
    "        self.decoder=Model(decoder_input, decoder_output, name='decoder')\n",
    "        \n",
    "    def _add_decoder_input(self):\n",
    "        return Input(shape=(self.latent_space_dim,), name='decoder_input')\n",
    "    \n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(self._shape_before_bottleneck)\n",
    "        ## This will multuply all the elements in the list, because the shape before bottleneck has like 3 elements\n",
    "        dense_layer = Dense(num_neurons, name='decoder_dense')(decoder_input)\n",
    "        return dense_layer\n",
    "    \n",
    "    def _add_reshape_layer(self, dense_layer):\n",
    "        ## we want to go back to the three d layer, we want to go the the encoder shape before flattening\n",
    "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
    "    \n",
    "    def _add_conv_transpose_layers(self, x):\n",
    "        ## Add convolutional transpose blocks\n",
    "        ## Loop through all the conv layers in reverse order and stop at the first layer\n",
    "        ## we need to ignore the first convolutional layer, and we want to do it in reverse order\n",
    "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
    "            #[0,1,2] ->[2,1]\n",
    "            x= self._add_conv_transpose_layer(layer_index, x)\n",
    "        return x\n",
    "    \n",
    "    def _add_conv_transpose_layer(self, layer_index, x):\n",
    "        layer_num = self._num_conv_layers - layer_index\n",
    "        conv_transpose_layer= Conv2DTranspose(filters=self.conv_filters[layer_index],\n",
    "                                              kernel_size=self.conv_kernels[layer_index],\n",
    "                                                strides=self.conv_strides[layer_index],\n",
    "                                                padding='same',\n",
    "                                                name=f'decoder_conv_transpose_{layer_num}')\n",
    "        x= conv_transpose_layer(x)\n",
    "        x=ReLU(name=f'decoder_relu_{layer_num}')(x)\n",
    "        x= BatchNormalization(name=f'decoder_bn_{layer_num}')(x)\n",
    "        return x\n",
    "         \n",
    "    def _add_decoder_output(self, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(filters=1, ## [24 24 1] height and width and num of channels we set to one because of number of channels in mnist\n",
    "                                              kernel_size=self.conv_kernels[0], ## getting the first kernel size\n",
    "                                                strides=self.conv_strides[0],\n",
    "                                                padding='same',\n",
    "                                                name=f'decoder_conv_transpose_layer_{self._num_conv_layers}')\n",
    "        x= conv_transpose_layer(x)\n",
    "        output_layer = activation('sigmoid', name='sigmoid_layer')(x)\n",
    "         \n",
    "         \n",
    "    \n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input()\n",
    "        conv_layers= self._add_conv_layers(encoder_input)\n",
    "        bottleneck = self._add_bottleneck(conv_layers)\n",
    "        self.encoder = Model(encoder_input, bottleneck, name='encoder')\n",
    "    \n",
    "    def _add_encoder_input(self):\n",
    "        return Input(shape=self.input_shape, name='encoder_input')\n",
    "        \n",
    "    ## This will create all convolutional blocks in encoder    \n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        x= encoder_input\n",
    "        for layer_index in range(self._num_conv_layers):\n",
    "            x = self._add_conv_layer(layer_index, x)\n",
    "        return x   \n",
    "    \n",
    "    def _add_conv_layer(self, layer_index, x):\n",
    "        layer_number= layer_index+1\n",
    "        ## Adds a convolutional block to a graph of layers, consisting of a conv 2d+ batch norm + relu \n",
    "        conv_layer = Conv2D(filters=self.conv_filters[layer_index],kernel_size=self.conv_kernels[layer_index],\n",
    "                            strides=self.conv_strides[layer_index], \n",
    "                            padding='same', name=f'encoder_conv_layer_{layer_number}')\n",
    "        x= conv_layer(x)\n",
    "        x= ReLU(name=f'encoder_relu_{layer_number}')(x)\n",
    "        x= BatchNormalization(name=f'encoder_bn_{layer_number}')(x)\n",
    "        return x\n",
    "    \n",
    " \n",
    "    \n",
    "    def _add_bottleneck(self, x):\n",
    "        ## Flatten the data and add bottleneck (a dense layer)\n",
    "        ## we want to store the shaoe of the data before flattening it, so we can use it in the decoder\n",
    "        self._shape_before_bottleneck= K.int_shape(x)[1:] #in our case [2 7 7 32] first -> batch size, second and third -> spatial dimensions, fourth -> number of channels\n",
    "        x= Flatten()(x)\n",
    "        ## Here it is like we instantiate a dense layer (the first paranthesis) and then we call it on x (with the second paranthesis)\n",
    "        x= Dense(self.latent_space_dim, name='encoder_output')(x)\n",
    "        return x\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    autoencoder= Autoencoder(input_shape=[28,28,1], conv_filters=[32,64,64,64], conv_kernels=[3,3,3,3], conv_strides=[1,2,2,1], latent_space_dim=2)\n",
    "    autoencoder.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## moving to the decoder\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfc00275c037e0322d65458631b9d66cd4b1daeaa2d39a92fa6d181b0d340908"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
